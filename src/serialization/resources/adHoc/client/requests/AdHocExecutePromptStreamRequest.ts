/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as serializers from "../../../../index";
import * as Vellum from "../../../../../api/index";
import * as core from "../../../../../core";
import { PromptRequestInputRequest } from "../../../../types/PromptRequestInputRequest";
import { VellumVariableRequest } from "../../../../types/VellumVariableRequest";
import { PromptParametersRequest } from "../../../../types/PromptParametersRequest";
import { AdHocExpandMetaRequestRequest } from "../../../../types/AdHocExpandMetaRequestRequest";

export const AdHocExecutePromptStreamRequest: core.serialization.Schema<
    serializers.AdHocExecutePromptStreamRequest.Raw,
    Vellum.AdHocExecutePromptStreamRequest
> = core.serialization.object({
    mlModel: core.serialization.property("ml_model", core.serialization.string()),
    inputValues: core.serialization.property("input_values", core.serialization.list(PromptRequestInputRequest)),
    inputVariables: core.serialization.property("input_variables", core.serialization.list(VellumVariableRequest)),
    parameters: PromptParametersRequest,
    blocks: core.serialization.list(core.serialization.lazy(() => serializers.PromptBlockRequest)),
    expandMeta: core.serialization.property("expand_meta", AdHocExpandMetaRequestRequest.optional()),
});

export declare namespace AdHocExecutePromptStreamRequest {
    interface Raw {
        ml_model: string;
        input_values: PromptRequestInputRequest.Raw[];
        input_variables: VellumVariableRequest.Raw[];
        parameters: PromptParametersRequest.Raw;
        blocks: serializers.PromptBlockRequest.Raw[];
        expand_meta?: AdHocExpandMetaRequestRequest.Raw | null;
    }
}
