/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Vellum from "../index";

/**
 * An ML Model that your Workspace has access to.
 */
export interface MlModelRead {
    id: string;
    /** The unique name of the ML Model. */
    name: string;
    /** The family of the ML Model. */
    family: Vellum.MlModelFamilyEnumValueLabel;
    /**
     * The organization hosting the ML Model.
     *
     * - `ANTHROPIC` - Anthropic
     * - `AWS_BEDROCK` - AWS Bedrock
     * - `AZURE_OPENAI` - Azure OpenAI
     * - `COHERE` - Cohere
     * - `CUSTOM` - Custom
     * - `FIREWORKS_AI` - Fireworks AI
     * - `GOOGLE` - Google
     * - `GOOGLE_VERTEX_AI` - Google Vertex AI
     * - `GROQ` - Groq
     * - `HUGGINGFACE` - HuggingFace
     * - `IBM_WATSONX` - IBM WatsonX
     * - `MOSAICML` - MosaicML
     * - `MYSTIC` - Mystic
     * - `OPENAI` - OpenAI
     * - `OPENPIPE` - OpenPipe
     * - `PYQ` - Pyq
     * - `REPLICATE` - Replicate
     */
    hostedBy: Vellum.HostedByEnum;
    /** The organization that developed the ML Model. */
    developedBy: Vellum.MlModelDeveloperEnumValueLabel;
    /**
     * The visibility of the ML Model.
     *
     * - `DEFAULT` - Default
     * - `PUBLIC` - Public
     * - `PRIVATE` - Private
     * - `DISABLED` - Disabled
     */
    visibility?: Vellum.VisibilityEnum;
    /** Configuration for how the ML Model was built. */
    buildConfig: Vellum.MlModelBuildConfig;
    /** Configuration for how to execute the ML Model. */
    execConfig: Vellum.MlModelExecConfig;
    /** Configuration for the ML Model's parameters. */
    parameterConfig: Vellum.MlModelParameterConfig;
    /** Configuration for how to display the ML Model. */
    displayConfig: Vellum.MlModelDisplayConfigLabelled;
}
