/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Vellum from "../../../..";

/**
 * @example
 *     {
 *         label: "label",
 *         name: "name",
 *         prompt: {
 *             promptBlockData: {
 *                 blocks: [],
 *                 version: 1
 *             },
 *             inputVariables: [{
 *                     key: "key"
 *                 }]
 *         },
 *         model: "model",
 *         parameters: {
 *             temperature: 1.1,
 *             maxTokens: 1,
 *             topP: 1.1,
 *             frequencyPenalty: 1.1,
 *             presencePenalty: 1.1
 *         }
 *     }
 */
export interface RegisterPromptRequestRequest {
    /** A human-friendly label for corresponding entities created in Vellum. */
    label: string;
    /** A uniquely-identifying name for corresponding entities created in Vellum. */
    name: string;
    /** Information about how to execute the prompt template. */
    prompt: Vellum.RegisterPromptPromptInfoRequest;
    /**
     * The initial LLM provider to use for this prompt
     *
     * * `ANTHROPIC` - Anthropic
     * * `AWS_BEDROCK` - AWS Bedrock
     * * `AZURE_OPENAI` - Azure OpenAI
     * * `COHERE` - Cohere
     * * `GOOGLE` - Google
     * * `HOSTED` - Hosted
     * * `MOSAICML` - MosaicML
     * * `OPENAI` - OpenAI
     * * `FIREWORKS_AI` - Fireworks AI
     * * `HUGGINGFACE` - HuggingFace
     * * `MYSTIC` - Mystic
     * * `PYQ` - Pyq
     * * `REPLICATE` - Replicate
     */
    provider?: Vellum.ProviderEnum;
    /** The initial model to use for this prompt */
    model: string;
    /** The initial model parameters to use for  this prompt */
    parameters: Vellum.RegisterPromptModelParametersRequest;
    /** Optionally include additional metadata to store along with the prompt. */
    meta?: Record<string, unknown>;
}
